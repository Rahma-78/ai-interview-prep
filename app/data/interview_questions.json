{"all_questions":[{"skill":"Machine Learning","questions":["Explain the bias-variance trade‑off and how it influences model selection in practice.","How do L1 (Lasso) and L2 (Ridge) regularization differ in their effect on model coefficients and feature selection?","When dealing with a highly imbalanced dataset, why might accuracy be misleading and which alternative metrics would you prioritize?","Describe how stratified cross‑validation works and why it is important for imbalanced classification problems.","What are the key considerations when choosing an imputation strategy for missing numerical versus categorical data?","Compare one‑hot encoding and label encoding for categorical variables; when might each be appropriate or problematic?","How does bagging (e.g., Random Forest) reduce variance, and how does boosting (e.g., XGBoost) address bias?","What are the main hyperparameters you would tune in a Gradient Boosting Machine and why do they impact model performance?","Discuss the challenges of deploying a reinforcement‑learning model in a safety‑critical domain such as autonomous driving.","Identify two ethical concerns related to machine‑learning models and propose practical steps to mitigate them."]},{"skill":"Deep Learning","questions":["Why do recurrent architectures like LSTMs and GRUs struggle with very long‑range dependencies compared to Transformer models?","Explain the self‑attention mechanism in Transformers and how it enables parallel processing of sequence data.","What is the role of positional encodings in a Transformer, and what would happen if they were omitted?","Contrast the training dynamics of RNN‑based models with Transformer‑based models in terms of gradient flow and computational efficiency.","How do multi‑head attention layers contribute to the expressive power of a Transformer?","Discuss the impact of model depth versus width in deep neural networks and how they relate to representation learning.","What are the primary purposes of dropout and layer normalization in deep networks, and how do they differ?","Describe how pre‑training and fine‑tuning are applied in models like BERT, and why this paradigm improves downstream performance.","In what scenarios might you still prefer a convolutional neural network over a Transformer for sequence modeling?","Outline potential risks of deploying large language models and strategies to address bias and hallucination." ]},{"skill":"Natural Language Processing","questions":["What are the trade‑offs between tokenizing text at the word level versus subword (e.g., Byte‑Pair Encoding) level?","How do lemmatization and stemming differ in preserving linguistic meaning, and when might each be preferable?","Explain how word embeddings such as Word2Vec capture semantic relationships, and why they outperform sparse bag‑of‑words representations.","What are the advantages of contextual embeddings (e.g., from BERT) over static embeddings like GloVe?","Describe how an attention mechanism improves sequence‑to‑sequence models for machine translation compared to plain encoder‑decoder RNNs.","Which evaluation metric would you choose for a sentiment‑analysis classifier on an imbalanced dataset, and why?","How do BLEU and ROUGE differ in assessing the quality of generated text, and what are their limitations?","Identify common sources of bias in NLP models and propose mitigation techniques during data collection and model training.","What challenges arise when applying NER systems to domain‑specific corpora (e.g., biomedical text), and how can they be addressed?","Discuss strategies for handling noisy or misspelled input text in real‑world NLP pipelines."]}]}